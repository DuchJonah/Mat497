{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i'm just importing everything I could possibly need here\n",
    "from scipy.signal import savgol_filter\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from itertools import zip_longest\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pylab\n",
    "import math\n",
    "from operator import itemgetter\n",
    "import re\n",
    "# Import module to split dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Import module for fitting\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVR\n",
    "from sklearn import linear_model\n",
    "import time\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler #https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html#sklearn.preprocessing.MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler #https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.RobustScaler.html\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import RANSACRegressor\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets unpickle our dataframes!\n",
    "only_347H = pd.read_pickle(\"./only_347H.pkl\")\n",
    "only_740H = pd.read_pickle(\"./only_740H.pkl\")\n",
    "\n",
    "#Maybe we could replace composition for 347H with dummy variables instead?\n",
    "only_347H.replace([0.04, 0.08], ['Low Carbon', 'High Carbon'], inplace=True)\n",
    "carbon = pd.get_dummies(only_347H['Composition'], drop_first=True)\n",
    "only_347H = pd.concat([only_347H, carbon], axis=1)\n",
    "only_347H.drop(['Composition'], axis=1, inplace=True)\n",
    "#print (only_347H.head)\n",
    "\n",
    "# Split data into 'X' features and 'y' target label sets\n",
    "X = only_740H[['Strain', 'Temperature', 'Time', 'FZ',\n",
    "                  'HAZ']]\n",
    "y = only_740H['Average']\n",
    "Z = only_347H[['Strain', 'Temperature', 'Time', 'FZ',\n",
    "                  'HAZ', 'Low Carbon']]\n",
    "i = only_347H['Average']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order is as follows:\n",
    "1. We need to preprocess our data. \n",
    "    * Easiest way is to use Standard scaler to transform our data.\n",
    "2. We need to choose our models\n",
    "    * Decision Tree, Random Forest some examples, but overall we would like to just use a whole list of them\n",
    "3. We need to use cross validation to refine our results\n",
    "    * Included in this is to use kernal trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we created a list of all the models and created the param_grid for use in GridSearchCV\n",
    "\n",
    "models = {'Kernal Ridge':KernelRidge(),                     #https://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html\n",
    "          'Decision Tree': DecisionTreeRegressor(),         #https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html\n",
    "          'Random Forest': RandomForestRegressor(),         #https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "          'K Neighbors': KNeighborsRegressor(),             #https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html\n",
    "          'Elastic Net': ElasticNet(),                      #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet\n",
    "          'RANSAC': RANSACRegressor(),                      #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.RANSACRegressor.html#sklearn.linear_model.RANSACRegressor\n",
    "          'Theil Sen': TheilSenRegressor(),                 #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.TheilSenRegressor.html#sklearn.linear_model.TheilSenRegressor\n",
    "          'Huber': HuberRegressor(),                        #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor\n",
    "          'SGD': SGDRegressor(),                            #https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor\n",
    "          'Gradient Boosting': GradientBoostingRegressor(), #https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor\n",
    "          'Gaussian Process': GaussianProcessRegressor(), #results were all over the place #https://scikit-learn.org/stable/modules/generated/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor\n",
    "          'PLS' : PLSRegression(),                          #https://scikit-learn.org/stable/modules/generated/sklearn.cross_decomposition.PLSRegression.html#sklearn.cross_decomposition.PLSRegression\n",
    "          'Multi-layer Perceptron': MLPRegressor(),         #https://sc3ikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor\n",
    "          'Support Vector':SVR()                            #https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html\n",
    "          #'Isotonic': IsotonicRegression(), #need to flatten X values, also only for all increasing or all decreasing #https://scikit-learn.org/stable/modules/generated/sklearn.isotonic.IsotonicRegression.html#sklearn.isotonic.IsotonicRegression\n",
    "          #'Logistic': LogisticRegression() #doesn't work for continuous\n",
    "         }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might imagine, the hyperparameter optimization would've taken me years to run, lets see how long it takes just to do one. We will be switching between the above hyperparameter with the full factorial and the below one with the single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_range = np.logspace(-2, 10, 5) #np.logspace(-2, 10, 13)\n",
    "gamma_range = [10e-5, 0.001, 0.01, 0.1, 1, 100]#np.logspace(-9, 3, 8) #np.logspace(-9, 3, 13)\n",
    "alpha_range = np.logspace(-7, 2, 8)\n",
    "epsilon_range = np.concatenate((np.logspace(-5, 0, 3),np.arange(2,11,2))) #np.logspace(-5, 0, 8)\n",
    "epsilon_range_v2 = np.logspace(-9, 0, 4) #np.logspace(-9, 0, 13)\n",
    "epsilon_range_v3 = np.linspace(1.01,20,5)\n",
    "max_iter_range = np.arange(100, 900, 200, dtype=int)\n",
    "n_estimator_range = np.arange(100, 400, 100, dtype=int)\n",
    "learning_rate_range = np.linspace(0.02,0.5,5)\n",
    "max_depth_range = np.linspace(3,10,4, dtype=int)\n",
    "\n",
    "hyperparameters = {'Kernal Ridge':[{'kernel':  ['rbf'], #The best parameters are {'alpha': 6.210526315789474e-05, 'gamma': 0.05, 'kernel': 'rbf'} with a score of 0.81\n",
    "                                    'alpha': np.linspace(0.00001, 0.0001, 10),                 \n",
    "                                    'gamma': np.arange(0.005, 0.08, 8)}],               \n",
    "                   'Decision Tree': [{'criterion': ['mse', 'mae'], #The best parameters are {'criterion': 'mse', 'max_depth': 13} with a score of 0.96\n",
    "                                      'max_depth' : [13]}],\n",
    "                   'Random Forest': [{'criterion': ['mse'], #The best parameters are {'criterion': 'mse', 'max_depth': 15.0, 'max_features': 'auto', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 387} with a score of 0.96\n",
    "                                      'n_estimators': np.linspace(50, 500, 8, dtype=int),\n",
    "                                      'max_depth' : np.linspace(5, 50, 5), \n",
    "                                      'max_features': ['auto', 'sqrt'],\n",
    "                                      'min_samples_leaf': [2, 4, 8, 20],\n",
    "                                      'min_samples_split': [5, 10, 15, 20] }],\n",
    "                   'K Neighbors': [{'n_neighbors': np.linspace(2,50,13,dtype=int), #The best parameters are {'n_neighbors': 14, 'weights': 'uniform'} with a score of 0.95\n",
    "                                    'weights':['uniform', 'distance']}],\n",
    "                   'Elastic Net': [{'alpha':alpha_range, \n",
    "                                    'max_iter':np.linspace(10,200 , 8, dtype=int), \n",
    "                                    'l1_ratio': np.arange(0.0, 1.0, 0.1)}],\n",
    "                   'RANSAC': True, #don't know if there's any hyperparameter types to adjust  #0.0147 sec\n",
    "                   'Theil Sen': [{'max_iter': max_iter_range}], #not a lot of hyperparameters to optimize #3.5645sec\n",
    "                   'Huber': [{'epsilon': epsilon_range_v3,                        #0.101\n",
    "                              'alpha':alpha_range, \n",
    "                              'max_iter': max_iter_range}],\n",
    "                   'SGD': [{'alpha': alpha_range,                          #0.08835\n",
    "                            'epsilon':epsilon_range_v2, #don't know if it's the same epsilon\n",
    "                            'loss': ['squared_loss', 'huber', 'epsilon_insensitive'],\n",
    "                            'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "                            'learning_rate': ['constant', 'optimal', 'invscaling']}],\n",
    "                   'Gradient Boosting': [{'alpha': np.logspace(-7, -1, 7),\n",
    "                                          'n_estimators': n_estimator_range,\n",
    "                                          'learning_rate': learning_rate_range,\n",
    "                                          'max_depth':max_depth_range,\n",
    "                                          'min_samples_leaf':[3,5,9,17], \n",
    "                                          'max_features':[0.1,0.3,0.5,1]}],\n",
    "                   'Gaussian Process' : [{'alpha': alpha_range, #'kernel': ['rbf'], #7.43\n",
    "                                         'n_restarts_optimizer': np.linspace(1,7,3, dtype=int)}] , #results were all over the place\n",
    "                   'PLS' : [{'n_components': np.linspace(2,10,6,dtype=int),\n",
    "                            'max_iter': max_iter_range}],    #not too much we can do here\n",
    "                   'Multi-layer Perceptron': [{'solver': ['lbfgs'],              #22.36\n",
    "                                               'epsilon': epsilon_range,\n",
    "                                               'max_iter': max_iter_range,\n",
    "                                               'alpha': alpha_range}], #'hidden_layer_sizes':np.arange(10, 15)\n",
    "                   'Support Vector': [{'kernel': ['rbf', 'linear','sigmoid'],\n",
    "                                       'degree': np.arange(2,6,dtype=int),\n",
    "                                       'gamma': gamma_range,\n",
    "                                       'C': C_range\n",
    "                                      }]\n",
    "                   #'Isotonic': IsotonicRegression(), #need to flatten X values, also only for all increasing or all decreasing\n",
    "                   #'Logistic': LogisticRegression() #doesn't work for continuous\n",
    "                  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:645: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\sklearn\\base.py:464: DataConversionWarning: Data with input dtype uint8, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kernal Ridge\n",
      "The best parameters are {'alpha': 1e-05, 'gamma': 0.005, 'kernel': 'rbf'} with a score of 0.84\n",
      "Process Time  5.118233400000008\n",
      "Decision Tree\n",
      "The best parameters are {'criterion': 'mse', 'max_depth': 13} with a score of 0.96\n",
      "Process Time  0.06483019999996031\n",
      "Random Forest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'criterion': 'mse', 'max_depth': 38.75, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 178} with a score of 0.96\n",
      "Process Time  211.97715990000006\n",
      "K Neighbors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'n_neighbors': 10, 'weights': 'distance'} with a score of 0.94\n",
      "Process Time  0.8581709000000046\n",
      "Elastic Net\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'alpha': 0.01389495494373139, 'l1_ratio': 0.9, 'max_iter': 10} with a score of 0.73\n",
      "Process Time  1.1484761000001527\n",
      "RANSAC\n",
      "347H RANSAC Model predicts: 0.6797293078567563\n",
      "Process Time  0.007327999999915846\n",
      "Theil Sen\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'max_iter': 100} with a score of -2.91\n",
      "Process Time  4.123500899999954\n",
      "Huber\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'alpha': 0.26826957952797276, 'epsilon': 10.504999999999999, 'max_iter': 100} with a score of 0.73\n",
      "Process Time  0.8058642999999392\n",
      "SGD\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'alpha': 0.26826957952797276, 'epsilon': 1e-09, 'learning_rate': 'invscaling', 'loss': 'squared_loss', 'penalty': 'l1'} with a score of 0.73\n",
      "Process Time  1.80296629999998\n",
      "Gradient Boosting\n",
      "The best parameters are {'alpha': 0.01, 'learning_rate': 0.5, 'max_depth': 5, 'max_features': 0.5, 'min_samples_leaf': 17, 'n_estimators': 200} with a score of 0.96\n",
      "Process Time  450.9139215\n",
      "Gaussian Process\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are {'alpha': 0.0007196856730011528, 'n_restarts_optimizer': 1} with a score of 0.84\n",
      "Process Time  8.315516000000116\n",
      "PLS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:45: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:28: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid number of components: 8",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\externals\\loky\\process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 567, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 225, in __call__\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 225, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 528, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"C:\\Users\\DuchJ\\Anaconda3\\lib\\site-packages\\sklearn\\cross_decomposition\\pls_.py\", line 263, in fit\n    self.n_components)\nValueError: Invalid number of components: 8\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-4488e451d206>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[1;31m#cv = train_test_split(test_size=0.2, random_state=42)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mgrid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhyperparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_train\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#this is typically done with the X and y not X_train and y_train\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The best parameters are %s with a score of %0.2f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 722\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    723\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1189\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params)\u001b[0m\n\u001b[0;32m    709\u001b[0m                                \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[1;32m--> 711\u001b[1;33m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    712\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    928\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    931\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    519\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    520\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    522\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    430\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    431\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 432\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    433\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    434\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid number of components: 8"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.set_palette('colorblind')\n",
    "\n",
    "#Standard Scaler is most common example: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "preprocess = {'Standard': StandardScaler(), 'Min Max': MinMaxScaler(), 'Robust': RobustScaler()} #we'll get around to this later\n",
    "material_selection = [('347H', Z, i),('740H', X, y)]\n",
    "counting_plots = 1\n",
    "\n",
    "for mat_name, inputs, outputs in material_selection:\n",
    "    #it may be training the data on the index\n",
    "    #inputs = pd.DataFrame.to_numpy(inputs)\n",
    "    #outputs = pd.DataFrame.to_numpy(outputs)\n",
    "    #we need standard scaler for the input and output data\n",
    "    scaler = StandardScaler()\n",
    "    input_data = scaler.fit_transform(inputs)\n",
    "    \n",
    "    input_train, input_test, output_train, output_test = train_test_split(input_data, outputs, test_size=0.3, random_state=100)\n",
    "    #scaler = StandardScaler()\n",
    "    #input_train = scaler.fit_transform(input_train)\n",
    "    #input_test = scaler.fit_transform(input_test)\n",
    "    fig = plt.figure(counting_plots)\n",
    "    ax = fig.add_axes([0,0,1,1])\n",
    "    title= \"Modeled Hardness vs. Actual Hardness for {}\".format(mat_name)\n",
    "    \n",
    "    if mat_name == '740H': features=5\n",
    "    elif mat_name == '347H':features=6\n",
    "    \n",
    "    for key in models.keys():\n",
    "        tic = time.clock()\n",
    "        print (key)\n",
    "        model = models[key]\n",
    "        hyperparam_grid = hyperparameters[key]\n",
    "        \n",
    "        if hyperparam_grid == True:\n",
    "            model.fit(input_train.reshape(-1, features), output_train)\n",
    "            print('{} {} Model predicts: {}'.format(mat_name, key, model.score(input_test.reshape(-1, features), output_test)))\n",
    "        else:\n",
    "            #input_data_y = scaler.fit_transform(y)\n",
    "            #cv = StratifiedShuffleSplit(n_splits=20, test_size=0.2, random_state=50) #ValueError: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.\n",
    "            cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=50)\n",
    "            #cv = train_test_split(test_size=0.2, random_state=42)\n",
    "            grid = GridSearchCV(model, param_grid=hyperparam_grid, cv=3, n_jobs=-1)\n",
    "            grid.fit(input_train.reshape(-1, features), output_train) #this is typically done with the X and y not X_train and y_train\n",
    "            print(\"The best parameters are %s with a score of %0.2f\" % (grid.best_params_, grid.best_score_))\n",
    "            \n",
    "        toc = time.clock()\n",
    "        print ('Process Time ', toc-tic)\n",
    "        #model.fit(input_train.reshape(-1, features), output_train)\n",
    "        #print('{} {} Model predicts: {}'.format(mat_name, key, model.score(input_test.reshape(-1, features), output_test)))\n",
    "        \n",
    "        '''\n",
    "        predicted_test = model.predict(input_test.reshape(-1, features))\n",
    "        \n",
    "        fig.suptitle(title)\n",
    "        ax.set_xlabel(\"Predicted Hardness\")\n",
    "        ax.set_ylabel(\"Actual Hardness\")\n",
    "        ax.scatter(predicted_test, output_test, label=key)\n",
    "        plt.legend()\n",
    "        #plt.show()\n",
    "        #'''\n",
    "        \n",
    "    \n",
    "    counting_plots+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the best models are: Decision Tree, Random Forest, K Neighbors with scores of 0.96, 0.96, 0.95 respectively. I gotta say I'm blown away, the other model scores are around 0.6 or 0.72 but this one works super well!\n",
    "\n",
    "Of course, we should be worried about overfitting. Fortunately, we only trained our model on the training data, so lets verify that we're not overfitting using our test data! I also threw in Kernel Ridge to see what a score of 0.81 looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "input_data = scaler.fit_transform(X)\n",
    "input_train, input_test, output_train, output_test = train_test_split(input_data, y, test_size=0.2, random_state=100)\n",
    "\n",
    "'''\n",
    "For 740H\n",
    "'Decision Tree': The best parameters are {'criterion': 'mse', 'max_depth': 13} with a score of 0.96\n",
    "'Random Forest': #The best parameters are {'criterion': 'mse', 'max_depth': 15.0, 'max_features': 'auto', 'min_samples_leaf': 4, 'min_samples_split': 10, 'n_estimators': 387} with a score of 0.96\n",
    "'K Neighbors': #The best parameters are {'n_neighbors': 14, 'weights': 'uniform'} with a score of 0.95\n",
    "kernal ridge : The best parameters are {'alpha': 6.210526315789474e-05, 'gamma': 0.05, 'kernel': 'rbf'} with a score of 0.81\n",
    "\n",
    "For 347H\n",
    "'Decision Tree': The best parameters are {'criterion': 'mse', 'max_depth': 13} with a score of 0.96\n",
    "'Random Forest': The best parameters are {'criterion': 'mse', 'max_depth': 38.75, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 5, 'n_estimators': 178} with a score of 0.96\n",
    "'K Neighbors': The best parameters are {'n_neighbors': 10, 'weights': 'distance'} with a score of 0.94\n",
    "Gradient Boosting: The best parameters are {'alpha': 0.01, 'learning_rate': 0.5, 'max_depth': 5, 'max_features': 0.5, 'min_samples_leaf': 17, 'n_estimators': 200} with a score of 0.96\n",
    "kernal ridge : The best parameters are {The best parameters are {'alpha': 1e-05, 'gamma': 0.005, 'kernel': 'rbf'} with a score of 0.84\n",
    "#'''\n",
    "\n",
    "dt = DecisionTreeRegressor(criterion = 'mse', max_depth = 13)\n",
    "rf = RandomForestRegressor(criterion = 'mse', max_depth = 15, max_features= 'auto', min_samples_leaf= 4, min_samples_split= 10, n_estimators= 387)\n",
    "kn = KNeighborsRegressor(n_neighbors= 14, weights= 'uniform')\n",
    "kr = KernelRidge(alpha= 6.210526315789474e-05, gamma= 0.05, kernel= 'rbf')\n",
    "\n",
    "dt.fit(input_train.reshape(-1, features), output_train)\n",
    "print('Decision Tree Model for 740H predicts: {}'.format(dt.score(input_test.reshape(-1, features), output_test)))\n",
    "predicted_test_dt = dt.predict(input_test.reshape(-1, features))\n",
    "rf.fit(input_train.reshape(-1, features), output_train)\n",
    "print('Random Forest Model for 740H: {}'.format(rf.score(input_test.reshape(-1, features), output_test)))\n",
    "predicted_test_rf = rf.predict(input_test.reshape(-1, features))\n",
    "kn.fit(input_train.reshape(-1, features), output_train)\n",
    "print('K Neighbors Model for 740H: {}'.format(kn.score(input_test.reshape(-1, features), output_test)))\n",
    "predicted_test_kn = kn.predict(input_test.reshape(-1, features))\n",
    "kr.fit(input_train.reshape(-1, features), output_train)\n",
    "print('Kernel Ridge Model for 740H: {}'.format(kr.score(input_test.reshape(-1, features), output_test)))\n",
    "predicted_test_kr = kr.predict(input_test.reshape(-1, features))\n",
    "\n",
    "fig = plt.figure(counting_plots)\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "    \n",
    "fig.suptitle(title)\n",
    "ax.set_xlabel(\"Predicted Hardness\")\n",
    "ax.set_ylabel(\"Actual Hardness\")\n",
    "ax.scatter(predicted_test_kr, output_test, label='Kernel Ridge')\n",
    "ax.scatter(predicted_test_dt, output_test, label='Decision Tree')\n",
    "ax.scatter(predicted_test_rf, output_test, label='Random Forest')\n",
    "ax.scatter(predicted_test_kn, output_test, label='K Neighbors')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# It works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Junk and Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "C_range = np.logspace(-2, 10, 5) #np.logspace(-2, 10, 13)\n",
    "gamma_range = np.logspace(-9, 3, 4) #np.logspace(-9, 3, 13)\n",
    "alpha_range = np.logspace(-9, 3, 4)\n",
    "epsilon_range = np.concatenate((np.logspace(-5, 0, 3),np.arange(2,11,2))) #np.logspace(-5, 0, 8)\n",
    "epsilon_range_v2 = np.logspace(-9, 0, 4) #np.logspace(-9, 0, 13)\n",
    "epsilon_range_v3 = np.linspace(1.01,20,5)\n",
    "max_iter_range = np.arange(100, 900, 200, dtype=int)\n",
    "n_estimator_range = np.arange(100, 400, 100, dtype=int)\n",
    "learning_rate_range = np.linspace(0.02,0.5,5)\n",
    "max_depth_range = np.linspace(3,10,4, dtype=int)\n",
    "\n",
    "#each of these models will call a different param_grid because some of them use different hyperparameters\n",
    "\n",
    "hyperparameters = {'Kernal Ridge':[{'kernel':  ['rbf'],#['rbf', 'poly', 'linear'], #49.8 sec to run just one value\n",
    "                                    'alpha': [10**-5], #alpha_range, \n",
    "                                    'gamma': [10**-5]}], #gamma_range}],\n",
    "                   #[{'kernel': ['rbf', 'poly'],'alpha': alpha_range,'gamma': gamma_range}, {'kernel': ['linear'],'alpha': alpha_range}],        #we're splitting this into two dictionaries because      #gamma doesn't work for a linear kernel  \n",
    "                   'Decision Tree': [{'criterion': ['mse', 'friedman_mse', 'mae'], #16.07 sec to run full hyperparameter\n",
    "                                      'max_depth' : np.arange(3, 15)}],\n",
    "                   'Random Forest': [{'criterion': ['mse', 'mae'], \n",
    "                                      'n_estimators': np.linspace(start = 200, stop = 2000, num = 10, dtype=int),\n",
    "                                      'max_depth' : np.arange(3, 15), \n",
    "                                      'max_features': ['auto', 'sqrt'],\n",
    "                                      'min_samples_leaf': [1, 2, 4],\n",
    "                                      'min_samples_split': [2, 5, 10] }],\n",
    "                   'K Neighbors': [{'n_neighbors': np.arange(2,15,dtype=int), \n",
    "                                    'weights':['uniform', 'distance']}],\n",
    "                   'Elastic Net': [{'alpha':alpha_range, \n",
    "                                    'max_iter':np.arange(1, 14, 3, dtype=int), \n",
    "                                    'l1_ratio': np.arange(0.0, 1.0, 0.1)}],\n",
    "                   'RANSAC': True, #don't know if there's any hyperparameter types to adjust\n",
    "                   'Theil Sen': [{'max_iter': max_iter_range}], #not a lot of hyperparameters to optimize\n",
    "                   'Huber': [{'epsilon': epsilon_range_v3, \n",
    "                              'alpha':alpha_range, \n",
    "                              'max_iter': max_iter_range}],\n",
    "                   'SGD': [{'alpha': alpha_range, \n",
    "                            'epsilon':epsilon_range, #don't know if it's the same epsilon\n",
    "                            'loss': ['squared_loss', 'huber', 'epsilon_insensitive'],\n",
    "                            'penalty': ['l2', 'l1', 'elasticnet'],\n",
    "                            'learning_rate': ['constant', 'optimal', 'invscaling']}],\n",
    "                   'Gradient Boosting': [{'alpha': alpha_range,\n",
    "                                          'n_estimators': n_estimator_range,\n",
    "                                          'learning_rate': learning_rate_range,\n",
    "                                          'max_depth':max_depth_range,\n",
    "                                          'min_samples_leaf':[3,5,9,17], \n",
    "                                          'max_features':[0.1,0.3,0.5,1]}],\n",
    "                   'Gaussian Process' : [{'kernel': ['rbf', 'poly','linear'],\n",
    "                                         'alpha': alpha_range,\n",
    "                                         'n_restarts_optimizer': np.linspace(1,10,5, dtype=int)}] , #results were all over the place\n",
    "                   'PLS' : [{'n_components': np.linspace(2,10,6,dtype=int),\n",
    "                            'max_iter': max_iter_range}],    #not too much we can do here\n",
    "                   'Multi-layer Perceptron': [{'solver': ['lbfgs'], \n",
    "                                               'epsilon': epsilon_range_v2,\n",
    "                                               'max_iter': np.linspace(500, 2000, 12, dtype=int),\n",
    "                                               'alpha': np.logspace(-9, 0, 10),\n",
    "                                               'solver': ['lbfgs', 'sgd', 'adam'] }], #'hidden_layer_sizes':np.arange(10, 15)\n",
    "                   'Support Vector': [{'kernel': ['rbf', 'poly','linear','sigmoid'],\n",
    "                                       'degree': np.arange(2,6,dtype=int),\n",
    "                                       'gamma': gamma_range,\n",
    "                                       'C': C_range\n",
    "                                      }]\n",
    "                   #'Isotonic': IsotonicRegression(), #need to flatten X values, also only for all increasing or all decreasing\n",
    "                   #'Logistic': LogisticRegression() #doesn't work for continuous\n",
    "                  }\n",
    "                  #'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "%%time\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "class MidpointNormalize(Normalize):\n",
    "\n",
    "    def __init__(self, vmin=None, vmax=None, midpoint=None, clip=False):\n",
    "        self.midpoint = midpoint\n",
    "        Normalize.__init__(self, vmin, vmax, clip)\n",
    "\n",
    "    def __call__(self, value, clip=None):\n",
    "        x, y = [self.vmin, self.midpoint, self.vmax], [0, 0.5, 1]\n",
    "        return np.ma.masked_array(np.interp(value, x, y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=50)\n",
    "    \n",
    "model = linear_model.LinearRegression()\n",
    "#C and gamma are only for classifiers, hyperparameters for linear regression are found here: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression\n",
    "#C_range = np.logspace(-2, 10, 13)\n",
    "#gamma_range = np.logspace(-9, 3, 13)\n",
    "fit_intercept_range = np.array([True, False])\n",
    "normalize_range = np.array([True, False])\n",
    "param_grid = dict(fit_intercept=fit_intercept_range, normalize=normalize_range)\n",
    "cv = StratifiedShuffleSplit(n_splits=20, test_size=0.2, random_state=50)\n",
    "#cv = train_test_split(test_size=0.2, random_state=42)\n",
    "grid = GridSearchCV(model, param_grid=param_grid, cv=cv, n_jobs=-1)\n",
    "grid.fit(X, y)\n",
    "print(\"The best parameters are %s with a score of %0.2f\"\n",
    "      % (grid.best_params_, grid.best_score_))\n",
    "      '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
